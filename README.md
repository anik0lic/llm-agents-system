# llm-agents-system
The problem concerns the development of an LLM agent system that functions as a problem-solving system in the domain of Oscar (Academy Awards) nominations and winners research. This system provides support to users in researching nominations, winners, and Oscar categories through structured task delegation and response generation.

In this project, the _**microsoft/Phi-3.5-mini-instruct**_ model was used.

Oscar data is stored in a JSON file containing information on categories, years, nominated films, and winners. The system must:
- Efficiently classify questions.
- Generate search parameters for data retrieval.
- Properly delegate tasks among agents.
- Generate a coherent response based on the data and context.

## Subproblem Descriptions
1. **Question Classification and Parameter Generation:** Identifying whether a question is related to the Oscars or general information and preparing parameters such as category, year, nominated film, or winner.
2. **Data Retrieval:** Searching the JSON file based on the defined parameters.
3. **Response Generation:** Creating a natural and coherent response for the user.
3. **General Information Response Generation:** Handling general knowledge questions.

## Description of Agents and Their Work

**1. Query Generator Agent:**
- Analyzes the question and classifies it as related or unrelated to the Oscars.
- If the question is related, it generates search parameters (e.g., category, year, winner).
- If not, it forwards the question to the general response agent.
  
**2. Oscar Fetching Agent:**
- Uses the parameters generated by the Query Generator Agent to search the JSON file.
- Retrieves information about nominations, winners, and films.
  
**3. Text Generator Agent:**
- Generates a response based on the retrieved data and the original user question.
- If data is not found, it responds based on context.
  
**4. General Answers Agent:**
- Generates responses for general knowledge questions based on user input.

## System Workflow Example
A user asks: _"Who won the Oscar for Best Picture in 2023?"_

The Query Generator Agent identifies the question as Oscar-related and generates a query:
```json
category: "Best Picture"
year: "2023"
won: true
```
The Oscar Fetching Agent searches the JSON file and finds the winner for the given category and year.

The Text Generator Agent generates a response:
**"The winner of the Oscar for Best Picture in 2023 was [Film Name]."**

## Conclusion
The developed system efficiently uses agents for question classification, data retrieval, and response generation. The system is modular and can be easily extended to support new types of questions or additional data sources. Future work may include search optimization, response personalization, and database expansion.
